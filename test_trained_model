import os
import gymnasium as gym
import numpy as np
import random
import json
from stable_baselines3 import SAC
import torch
from datetime import datetime

def test_model_with_render(env, model, n_episodes, max_total_steps=100_000):
    """
    Prueba el modelo entrenado en el entorno con renderizado para visualizar la política.
    """
    total_reward = 0
    total_steps = 0

    for episode in range(n_episodes):
        obs, _ = env.reset()
        done = False
        episode_reward = 0
        episode_steps = 0

        print(f"--- Episode {episode + 1} ---")
        while not done and episode_steps < max_total_steps:
            env.render()  # Renderizar el entorno
            action, _ = model.predict(obs, deterministic=False)  # Política aprendida
            obs, reward, done, truncated, _ = env.step(action)
            episode_reward += reward
            episode_steps += 1

            if done or truncated:
                print(f"Episode {episode + 1} finished with reward: {episode_reward} and steps: {episode_steps}")
                break

        total_reward += episode_reward
        total_steps += episode_steps

    avg_reward = total_reward / n_episodes
    avg_steps = total_steps / n_episodes
    print(f"\nAverage Reward: {avg_reward}")
    print(f"Average Steps: {avg_steps}")

if __name__ == "__main__":
    # Configuración del entorno
    env_name = "Ant-v5"
    
    # Uso de rutas relativas
    current_dir = os.path.dirname(os.path.realpath(__file__))  # Directorio actual donde se ejecuta el script
    xml_path = os.path.join(current_dir, 'mujoco_menagerie', 'unitree_go1', 'scene.xml')  # Ruta relativa
    env = gym.make('Ant-v5', 
                   xml_file=xml_path,
                   forward_reward_weight=1,
                   ctrl_cost_weight=0.1,
                   contact_cost_weight=0.5,
                   healthy_reward=0.1,
                   main_body=1,
                   healthy_z_range=(0.2, 1),
                   reset_noise_scale=0,
                   frame_skip=1,
                   max_episode_steps=1_000,render_mode = 'human')

    # Rutas relativas para modelos y logs
    model_dir = os.path.join(current_dir, 'sac_ant_best_model')
    os.makedirs(model_dir, exist_ok=True)

    # Buscar el modelo más reciente en el directorio
    model_files = [f for f in os.listdir(model_dir) if f.endswith(".zip")]
    if model_files:
        # Encontrar el modelo más reciente basado en el nombre
        model_files.sort(reverse=True)
        latest_model_path = os.path.join(model_dir, model_files[0])
        print(f"Found existing model: {latest_model_path}. Loading model...")
        model = SAC.load(latest_model_path)
    else:
        raise FileNotFoundError(f"No trained model found in {model_dir}. Please train a model first.")

    # Probar el modelo con renderizado
    print("Testing the model with rendering...")
    test_model_with_render(env, model, n_episodes=5)

    # Cerrar el entorno
    env.close()
